# -*- coding: utf-8 -*-
"""FP2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p0zRqV665q1FnyH8ZQs01vrhbzTuQJB8
"""

import pandas as pd
import numpy as np
from nltk.corpus import stopwords
import nltk
import re
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import euclidean_distances

# !pip install docx

# import docx

pip install tika

from google.colab import drive

drive.mount('/content/drive/')
#drive.mount('/content/drive/MyDrive/ColabNotebooks/FP2')

path = "/content/drive/MyDrive/ColabNotebooks/FP2"

from tika import parser 
# pip install tika



raw_jd = parser.from_file('/content/drive/MyDrive/ColabNotebooks/FP2/JD.pdf')
jd=raw_jd['content']

raw_resume_ds1 = parser.from_file('/content/drive/MyDrive/ColabNotebooks/FP2/1.pdf')
resume1=raw_resume_ds1['content']

raw_resume_ds2 = parser.from_file('/content/drive/MyDrive/ColabNotebooks/FP2/2.pdf')
resume2=raw_resume_ds2['content']

raw_resume_PM = parser.from_file('/content/drive/MyDrive/ColabNotebooks/FP2/PM.pdf')
resume_PM=raw_resume_PM['content']

#resume2

documents=[resume1,resume2,resume_PM,jd]

#documents

# Sample corpus
#documents = ['\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResponsibilities \n• Responsible for the design, development, documentation of the MDM procedures \n\n• Analyzes and defines detailed MDM processes, tasks, data flows, and dependencies \n\n• Define and Maintain the processes and procedures for Data Governance for the SC enterprise. Drive issue \n\nresolution across different organization constituencies. \n\n• Ensure alignment of data quality activities to business and Data Governance strategies. \n\n• Provide guidance to project teams around metrics requirements and tracking to monitor data quality \n\nefforts. \n\n•  Communicate effectively across stakeholder groups to define priorities, schedules and resources needed. \n\nSupport and maintain dashboard creation for data integrity tracking. \n\n• Lead internal process improvements, create process documentation and bring different and new \n\nperspectives to solving problems. \n\n•  Partner with Product Data Management Engineering, Sourcing, Supply Chain Ops & Planning groups to \n\nensure accurate & timely maintenance of Product & SC data. \n\n• Monitor internal requests and establish standard processes to improve efficiencies and provide internal \n\ncustomer support. \n\n• Work independently to balance multiple tasks and competing priorities simultaneously. \n\n• Effectively run cross-organizational meetings to lead teams to consensus and/or decisions to meet \n\ndeliverables. \n\n• Custodian for SC data across business data domains, ensuring that the data is properly defined and used \n\nwith a deep understanding of data governance principles and processes. \n\n• Drive compliance to established standards and metrics. \n\n• Communicate and enforce data policies, rules, and standards. \n\n• Perform data analysis, leveraging large data sets to develop insights & support data management \n\ndecisions, track, monitor, and publish cross data quality reporting/scorecards. \n\n• Provide clear, effective, and timely communications. \n\n• Maintain flexibility with regard to work hours to support worldwide communications as needed. \n\nQualifications \nRequired Qualification: \n\n  \n\n• Bachelor’s degree in Supply Chain/IT operations, Data Science, Engineering, or equivalent \n\n• 5+ years experience in SC/Product Data operations/management/analysis \n\n• Working design & development experience in Excel, data analytics & reporting tools incl. Power BI \n\n\n\nDesired / Preferred: \n\n• Experience with Product Lifecycle Management and Supply Chain Data Management systems \n\n• Ability to manage numerous competing priorities in a flexible and fast-paced environment \n\n• Ability to successfully collaborate across multiple teams \n\n• Capable of strong individual contribution and influence across non-direct reporting teams \n\n• Problem solving capability with a calm demeanor \n\n• Ability to maintain good judgment in fast-paced, high-stress environments \n\n• Cloud / high tech supply chain experience is a plus \n\n• Strong written and oral communication, organization, and time-management skills with high attention to \n\ndetail \n\n• Analytical aptitude while able to maintain a broader perspective, identifies and evaluates opportunities to \n\nenhance the business through cross-group collaboration. \n\n• Enterprise data management, integration, analytical, and data quality skills \n\n• Understands data pipelines, data cleansing, data transformations and other analytical techniques required \n\nfor data manipulation \n\n• Experience as a data engineer, BI developer, or data analyst \n\n \n\n\n',
#'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMicrosoft Word - Resume_Zehai Wang _2020_DS.docx\n\n\nZehai (Steve)Wang \nLinkedIn: https://www.linkedin.com/in/zehai-wang/   \n\n45 Stuart St., Boston, MA 02116 \n(518) 961-6851 \n\nzehai.wang@gmail.com \n\nSummary \no Data scientist with 3 years’ experience building real-life and competition machine learning models. \no Experience managing DS project and handling relationship with stakeholders and engineer collaborators. \no Ph.D. in physics-based computational modeling with working knowledge of high-performance computing. \no Published 5 top-tier peer-reviewed journal papers and oral presentations. \n\nWork Experience \nWayfair LLC, Boston, MA \nData Scientist (manager)           Feb 2020 – Now \n\no Led data science style prediction functional team defining road map and handling stakeholder relationship. \no Built machine learning service supporting product style prediction and exclusive brands assortment for over \n\n100 classes (Docker/Airflow/Python/Keras). \nData Scientist                        Apr 2019 – Feb 2020 \n\no Built (from zero to production) a product selection gap pipeline supporting catalog continuous expansion. \no Built supervised product matching models using fasttext, tf-idf embedding and fuzzy-wuzzy similarity \n\nfeatures reducing gap prediction false positive rate from 50% to 10%. \no Leveraged duplication logics in Hadoop distributed ecosystem enabling scalable deduplication between over \n\n100 Million competitor and over 20 Million Wayfair products (PySpark/Airflow/Python). \nRensselaer Polytechnic Institute, Troy, NY \nResearch Associate – Data Science (with OptumLabs)                        Nov 2018 – Apr 2019 \n\no Implemented data ETL pipeline exploring over 200 million patient insurance claims and EMRs (SQL). \no Designed hypothesis testing and identified a new feature explained 95% variance of fracture (Python/SciPy). \no Led proposal writing and got $300,000+ data mining grant from National Institute of Health. \n\nComputational Research Assistant                                  Sep 2014 – Dec 2018 \no Computational mathematics/ physics-based modeling for biomedical applications.  \no Created and maintained open-source tool-kits for stochastic structure modeling in FEM software. \n\nRelevant Projects \nElo Merchant Category Recommendation – Kaggle, top 4% out of 3448 teams (Profile)         Dec 2018- Feb 2019                                                                                                      \nThis project tried to predict customer loyalty score in order to target discounts to specific users. \n\no Conducted feature engineering based on credit card transaction and merchant categories (Python/Pandas). \no Applied ensembled modeling, the first level is a gradient boosting tree regression model, second level model \n\nis logistic regression classifier to identify outliers (Sci-kit Learn/ Light-GBM). \n\nEducation \nPh.D. in Aeronautical Engineering  Rensselaer Polytechnic Institute, major 4.0/4.0      Dec 2018 \nM.S.   in Applied Mathematics   Rensselaer Polytechnic Institute, 3.74/4.0            May 2018                                                                                                                             \nM.S.   in Aircraft Design  Beijing Univ. of Aero. & Astro., China, 3.78/4.0                     Jul 2014                \nB.S.    in Manufacturing Engineering      Tongji University, China, 91.2/100                 Jul 2011   \n\nSkills \nProgramming: \nTools/ Cloud: \nCourses: \n \n\nPython, SQL, Spark, C++, Scala, Java, Linux/Shell bash. \nAirflow, Docker, Git, Jupyter Notebook, Pandas, Scikit-Learn, Keras \nAI, Big Data & Health, Machine Learning, Deep Learning, Database System, Nonlinear \nProgramming, Computational Linear Algebra, Stochastic Differential Equations. \n\n \n\n\n',
#             '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMILI SHAH \n206-591-6095\n\nE-mail: milishah224@gmail.com LinkedIn: linkedin.com/in/milishah224 GitHub: github.com/itsmilishah \n\nEDUCATION UNIVERSITY OF MASSACHUSETTS AMHERST \nMaster’s in Computer Science, September 2017 – May 2019 \n \n\nNIRMA UNIVERSITY \nBachelor of Technology, Computer Engineering, August 2013 – May 2017 \n\n  \n\nEXPERIENCE AMAZON | Software Development Engineer \nAugust 2019 - Present \n•  As part of the ProductGraph team, developing workflows that build a knowledge graph and use it to clean, correct \nand extract new information for supporting several use cases of Amazon Music. \n•  Developing and maintaining pipelines in Spark that run on AWS Elastic MapReduce and scale out to ingest, process \nand disambiguate billions of facts and millions of entities on a daily basis. \n \n\nVIASAT | Software Engineering Intern \nMay 2018 – August 2018 \n•  Developed a PyTorch based pipeline in AWS that provides real-time predictions about when multiple airplanes will \nfall under the coverage area of the same satellite, for data multicasting optimization. \n• Resulted in projected savings of 500 GB in data transfer per month for in-flight WiFi services. \n \n\nMORGAN STANLEY | Technology Analyst Intern \nMay 2016 – July 2016 \n• Developed a system to automate level-1 support for Java developers of Morgan Stanley. \n• Achieved good qualitative performance, with implementation of the system in Python using NLTK, that clusters \nnoisy e-mail queries to analyse topics and mines old discussions and wiki pages to answer new e-mails. \n \n\nPROJECTS MACHINE READING COMPREHENSION QUESTION ANSWERING \nFebruary 2018 – May 2018 \n• Built models for Question Answering on SQuAD based on BiDAF and transformers in PyTorch. Fused linguistic \ninformation, obtained using spaCy, into the models. \n• Achieved an F1 score of 72.14% by adding a dependency parse layer, implemented using a transformer, to BiDAF - \nan improvement over AllenAIs score of 71.49%.  \n \n\nIMAGE TAG PREDICTION USING KNOWLEDGE GRAPHS \nSeptember 2018 – December 2018 \n• Developed models to incorporate semantic information into image tagging models by exploiting tag relations from \nKnowledge Graphs like ConceptNet. \n• Implemented techniques that incorporate information from GloVe embeddings using transfer learning and \nKnowledge Graphs using multi-task learning. \n \n\nCOMPLEX EMBEDDINGS FOR UNIVERSAL SCHEMA \nNovember 2017 – December 2017 \n• Developed a Knowledge Graph construction model that uses complex embeddings to capture asymmetry in rowless \nUniversal Schema.  \n• Achieved an improved mean reciprocal rank of 0.33 in relation extraction, while joint learning from unstructured \ntext and KBs, with implementation in Tensorflow. \n \n\nGOOGLE: LARGE-SCALE COMMONSENSE AS LEXICAL ENTAILMENT \nJanuary 2018 – April 2018 \n• Performed crowdsourcing experiments in collaboration with Google using CrowdFlower aimed towards \nconstructing a common-sense hypernym taxonomy. \n• Constructed an Elasticsearch database of 205 million sentences to use in the experiments as questions. \n \n\nCOURSES Natural Language Processing, Advanced Machine Learning, Deep Learning, Algorithms for Data Science, \nReinforcement Learning, Computer Vision, Advanced Data Structures, Databases, Operating Systems \n \n\nTECHNICAL \nSKILLS \n\nLanguages: Python, Java, R, C, C++, Javascript \nModules/Frameworks: Tensorflow, PyTorch, Keras, spaCy, CoreNLP, Spark, Gensim, FastText, RDFLib, Elasticsearch, \nAllenNLP, Scikit-learn \nOther: AWS, Hadoop, SQL \n\n \n \n\n \n\nmailto:milishah224@gmail.com\nhttp://www.linkedin.com/in/milishah224\nhttp://www.github.com/itsmilishah\n\n',
#'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nproject manager resume sample \n\n\n           \n\n           \n\n           \n\n           \n\n    \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nSarah Collins \nProject Manager Resume \n\nDayjob Ltd, The Big Peg, 120 Vyse Street, Birmingham B18 6NF  \n\nTel: 0870 061 0121     Mobile: 0777 777 7777     Email: info@dayjob.com  \n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n  \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nc \n\nPersonal profile \n\nA multi-skilled professional with a superb track \n\nrecord of managing complex functional projects in \n\nvarious environments. Able to manage stakeholder \n\nexpectations and willing to take full responsibility for \n\nthe delivering of project objectives. Sarah is an easy \n\ngoing individual who enjoys challenging and diverse \n\nroles and is confident working with technical experts \n\nfrom any industry.  \n\n \n\nPresently looking to join a company that rewards \n\neffort and initiative, whilst at the same time \n\nproviding plenty of progression and development \n\nopportunities to its employees.    \n\n \n\nAcademic qualifications \n\nBA (Hons) Project Management      \n\nPRNCE 2 \n\nA’ Levels:  Maths (C)  English (A)  Physics (A) \n\n \n\nAreas of expertise \n\nManagement knowledge Financial management \n\nPlanning & organising Business planning \n\nProgress reports Risk management \n\n \n\nWork experience \n\nPROJECT MANAGER \n\nConstruction Company           May 2010 - Present \n\nResponsible for delivering projects against agreed \n\nscope, budget, schedule & customer expectations. \n\nDoing this whilst supervising, directing & motivating \n\nteams of multi-discipline contractors & employees. \n\n \n\nDuties:  \n\n\uf0b7 Monitoring project risks and scope creep to \n\nidentify potential problems and proactively \n\nidentifying solutions to address them in advance.  \n\n\uf0b7 Escalating promptly any issues that may impact \n\noperations. \n\n\uf0b7 Producing stage plans, highlight reports, risk logs, \n\nrequests for change etc \n\n\uf0b7 Providing strategic direction during the \n\nimplementation stages.   \n\n\uf0b7 Managing client expectations by ensuring the \n\ndelivery of the highest quality service \n\n\uf0b7 Acting on client feedback. \n\n\uf0b7 Monitoring staff & team performance. \n\n   \n\n \n\n \n\nEXPERIENCE & KEY COMPETENCIES \n\n \n\nProject Management skills \n\n \n\n\uf0b7 Commercial awareness & business acumen. \n\n\uf0b7 Experience of working in a multi vendor \n\nenvironments.  \n\n\uf0b7 Good conflict management and prioritisation \n\nskills. \n\n\uf0b7 Manage technical process and resolve technical \n\nissues. \n\n\uf0b7 Can understand and document project \n\nrequirements and dependencies.  \n\n\uf0b7 Excellent documentation & report writing skills. \n\n\uf0b7 Experience of managing change within CRM, \n\nMarketing and Finance systems.   \n\n\uf0b7 Demonstrated ability to work with and support \n\ncross-functional project teams. \n\n\uf0b7 Ability to manage multiple projects \n\nsimultaneously and under pressure. \n\n\uf0b7 Strong attention to detail and focus on task \n\ncompletion.  \n\n\uf0b7 Internal and External Stakeholder Management. \n\n \n\nPersonal attributes \n\n \n\n\uf0b7 Team leader capable of motivating staff. \n\n\uf0b7 Ability to get on with all levels of people and \n\npossess strong relationship building skills. \n\n\uf0b7 Superb communication skills and able to \n\narticulate technical jargon to a non technical \n\naudience.  \n\n\uf0b7 Ability to gain results through others. \n\n\uf0b7 Knowledge of project management methods. \n\n\uf0b7 Able to continuously meet targets and surpass \n\nexpectations.  \n\n\uf0b7 Articulate and well presented.  \n\n\uf0b7 Having the necessary drive and enthusiasm \n\nrequired for a tough competitive industry. \n\n \n\nReferences \n\nAvailable on request.  \n\n \n\nPersonal \n\nDriving license:  Yes  \n\nNationality:   British \n\nLanguages:  German, French \n\n \n\n \n\n \n\n \n\nmailto:info@dayjob.com\n\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nCopyright information - Please read \n\n \n\n© This project manager resume template is the copyright of Dayjob Ltd 2012. Jobseekers may download and \nuse this example for their own personal use to help them create their own unique project manager resume. \n\nYou are most welcome to link to any page on our site www.dayjob.com. However this sample must not be \ndistributed or made available on other websites without our prior permission. For any questions relating to \n\nthe use of this resume template please email: info@dayjob.com.   \n\n \n\nhttp://www.dayjob.com/content/resume-templates-326.htm\nhttp://www.dayjob.com/\nmailto:info@dayjob.com\n\n'

#]

pd.set_option('display.max_colwidth', 0)
pd.set_option('display.max_columns', 0)

documents_df=pd.DataFrame(documents,columns=['documents'])

"""### Corpus Dataframe"""

#documents_df

import nltk
nltk.download("stopwords")

# removing special characters and stop words from the text
stop_words_l=stopwords.words('english')
documents_df['documents_cleaned']=documents_df.documents.apply(lambda x: " ".join(re.sub(r'[^a-zA-Z]',' ',w).lower() for w in x.split() if re.sub(r'[^a-zA-Z]',' ',w).lower() not in stop_words_l) )

#documents_df

"""#### Tf-idf vectors"""

tfidfvectoriser=TfidfVectorizer(max_features=64)
tfidfvectoriser.fit(documents_df.documents_cleaned)
tfidf_vectors=tfidfvectoriser.transform(documents_df.documents_cleaned)

#tfidf_vectors.shape

# Every vector is already normalised to have unit L2 norm
# np.linalg.norm(tfidf_vectors[0],ord=2)

tfidf_vectors=tfidf_vectors.toarray()
#print (tfidf_vectors[0])

"""##### Every document has been converted into a 64 dimensional vector. As we set the max_features=64

### Pairwise similarity

##### Pairwise cosine similarity would just be the dot product of the vectors becasue tf-idf vectors from sklearn are already normalized and L2 norm of these vectors is 1. So denominator of cosine similiarity formula is 1 in this case.
"""

pairwise_similarities=np.dot(tfidf_vectors,tfidf_vectors.T)
pairwise_differences=euclidean_distances(tfidf_vectors)

#print (tfidf_vectors[0])
#print (pairwise_similarities.shape)
#print (pairwise_similarities[0][:])

"""##### similarity is highest, 1 at index 0 becasue they are the same documents"""

def most_similar(doc_id,similarity_matrix,matrix):
    print (f'Document: {documents_df.iloc[doc_id]["documents"]}')
    print ('\n')
    print (f'Similar Documents using {matrix}:')
    if matrix=='Cosine Similarity':
        similar_ix=np.argsort(similarity_matrix[doc_id])[::-1]
    elif matrix=='Euclidean Distance':
        similar_ix=np.argsort(similarity_matrix[doc_id])
    for ix in similar_ix:
        if ix==doc_id:
            continue
        print('\n')
        print (f'Document: {documents_df.iloc[ix]["documents"]}')
        print (f'{matrix} : {similarity_matrix[doc_id][ix]}')

most_similar(0,pairwise_similarities,'Cosine Similarity')

most_similar(0,pairwise_differences,'Euclidean Distance')

"""#### word2vec embeddings

#### BERT model
"""

from sentence_transformers import SentenceTransformer

sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')

document_embeddings = sbert_model.encode(documents_df['documents_cleaned'])

pairwise_similarities=cosine_similarity(document_embeddings)
pairwise_differences=euclidean_distances(document_embeddings)

most_similar(0,pairwise_similarities,'Cosine Similarity')

from keras.preprocessing.text import Tokenizer
import gensim
# from keras.preprocessing.sequence import pad_sequences
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

most_similar(0,pairwise_differences,'Euclidean Distance')

from gensim.models.doc2vec import Doc2Vec, TaggedDocument

tagged_data = [TaggedDocument(words=word_tokenize(doc), tags=[i]) for i, doc in enumerate(documents_df.documents_cleaned)]

# pip install word_tokenize

model_d2v = Doc2Vec(vector_size=100,alpha=0.025, min_count=1)
  
model_d2v.build_vocab(tagged_data)

for epoch in range(100):
    model_d2v.train(tagged_data,
                total_examples=model_d2v.corpus_count,
                epochs=model_d2v.epochs)

document_embeddings=np.zeros((documents_df.shape[0],100))

for i in range(len(document_embeddings)):
    document_embeddings[i]=model_d2v.docvecs[i]

pairwise_similarities=cosine_similarity(document_embeddings)
pairwise_differences=euclidean_distances(document_embeddings)

most_similar(0,pairwise_similarities,'Cosine Similarity')

# tokenize and pad every document to make them of the same size
tokenizer=Tokenizer()
tokenizer.fit_on_texts(documents_df.documents_cleaned)
tokenized_documents=tokenizer.texts_to_sequences(documents_df.documents_cleaned)
tokenized_paded_documents=pad_sequences(tokenized_documents,maxlen=64,padding='post')
vocab_size=len(tokenizer.word_index)+1

print (tokenized_paded_documents[0])

# loading pre-trained embeddings, each word is represented as a 300 dimensional vector
import gensim
W2V_PATH="/Users/varunchaudhary/Documents/Varun Docs/Medium/GoogleNews-vectors-negative300.bin.gz"
model_w2v = gensim.models.KeyedVectors.load_word2vec_format(W2V_PATH, binary=True)